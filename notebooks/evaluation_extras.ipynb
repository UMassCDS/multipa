{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Additional Modeling Pipelines\n",
    "We should also compare performance in the evaluation data with other readily available phonetic transcription options, to determine whether fine-tuning your own model is worth the effort. \n",
    "The two options we consider here are: \n",
    "- [Allosaurus](https://github.com/xinjli/allosaurus) is a pre-trained universal phone recognizer that claims to recognize phones in more than 2000 languages. \n",
    "- [Whisper](https://openai.com/index/whisper/) is the state-of-the-art sequence-to-sequence speech recognition model released by OpenAI. Details about the different model releases are available at https://github.com/openai/whisper/blob/main/model-card.md. There are multilingual and English fine-tuned versions. We follow these models with grapheme to phoneme conversion using Epitran.\n",
    "\n",
    "These evaluations only need to be run and computed once. \n",
    "\n",
    "## Additional installation step for Epitran\n",
    "To use Epitran for English, you also need to install https://github.com/festvox/flite. See the Epitran note at https://github.com/dmort27/epitran?tab=readme-ov-file#installation-of-flite-for-english-g2p.  I installed Flite on my mac:\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "import datasets\n",
    "import epitran\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from multipa.data_utils import load_buckeye_split, clean_text\n",
    "from multipa.evaluation import ModelEvaluator, preprocess_test_data, write_detailed_prediction_results, DETAILED_PREDICTIONS_CSV_SUFFIX, PREDICTION_KEY\n",
    "\n",
    "VERBOSE_RESULTS_DIR = Path(\"../data/evaluation_results/detailed_predictions\")\n",
    "AGGREGATE_METRICS_CSV = Path(\"../data/evaluation_results/aggregate_metrics/epitran_allosaurus_eval.csv\")\n",
    "EDIT_DIST_DIR = Path(\"../data/evaluation_results/edit_distances/\")\n",
    "\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # For HuggingFace dataset map and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    model = allosaurus.app.read_recognizer(model)\n",
    "    start = time.time()\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "        model_predictions.append({PREDICTION_KEY: prediction})\n",
    "    end = time.time()\n",
    "    print(\"Eval time in seconds:\", end-start)\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: clean_text(x, text_key=PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, generate_kwargs={\"language\": \"english\"}\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    start = time.time()\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        result = epi.transliterate(pred)\n",
    "        ipa_predictions.append({PREDICTION_KEY: result})\n",
    "    end = time.time()\n",
    "    print(\"Eval time in seconds:\", end-start)\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: clean_text(x, text_key=PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio', '__index_level_0__'],\n",
      "    num_rows: 5079\n",
      "})\n",
      "{'utterance_id': 's2501a_Utt0', 'duration': 0.925981, 'buckeye_transcript': 'f ao r f ay v', 'text': 'four five', 'ipa': 'f ɔ ɹ f aɪ v', 'speaker_id': 'S25', 'speaker_gender': 'f', 'speaker_age_range': 'o', 'interviewer_gender': 'm', 'file_path': 'data/buckeye/test/s2501a_Utt0.wav', 'audio': {'bytes': None, 'path': '/Users/virginia/workspace/multipa/data/buckeye/test/s2501a_Utt0.wav'}, '__index_level_0__': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5079/5079 [00:00<00:00, 14490.10 examples/s]\n",
      "Filter: 100%|██████████| 5079/5079 [00:02<00:00, 1950.96 examples/s]\n",
      "Filter: 100%|██████████| 5079/5079 [00:01<00:00, 3509.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio', '__index_level_0__'],\n",
      "    num_rows: 5079\n",
      "})\n",
      "{'utterance_id': 's2501a_Utt0', 'duration': 0.925981, 'buckeye_transcript': 'f ao r f ay v', 'text': 'four five', 'ipa': 'fɔɹfaɪv', 'speaker_id': 'S25', 'speaker_gender': 'f', 'speaker_age_range': 'o', 'interviewer_gender': 'm', 'file_path': 'data/buckeye/test/s2501a_Utt0.wav', 'audio': {'path': '/Users/virginia/workspace/multipa/data/buckeye/test/s2501a_Utt0.wav', 'array': array([-0.00997925, -0.01052856, -0.00958252, ...,  0.00085449,\n",
      "        0.00061035,  0.00042725]), 'sampling_rate': 16000}, '__index_level_0__': 0}\n",
      "Test data without speech\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio', '__index_level_0__'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_data = load_buckeye_split(\"../data/buckeye\", \"test\")\n",
    "# Snippet of transcriptions\n",
    "# Note that there don't appear to be any non-empty transcriptions,\n",
    "# so this notebook skips looking at hallucinations\n",
    "print(\"Data Preview\")\n",
    "print(input_data)\n",
    "print(input_data[0])\n",
    "\n",
    "non_empty_test_data, empty_test_data = preprocess_test_data(input_data, is_remove_space=True)\n",
    "\n",
    "print(\"Test data with speech transcriptions\")\n",
    "print(non_empty_test_data)\n",
    "print(non_empty_test_data[0])\n",
    "print(\"Test data without speech\")\n",
    "print(empty_test_data)\n",
    "\n",
    "model_evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pipeline and downloading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with openai/whisper-large-v3-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed language=english, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=english.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    # Epitran\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, non_empty_test_data)\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[PREDICTION_KEY], non_empty_test_data[\"ipa\"]\n",
    "    )\n",
    "    write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, non_empty_test_data, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name,EDIT_DIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and phone inventory to test\n",
    "allosaurus_models = [\"uni2005\", \"eng2102\"]\n",
    "phone_inventory = [\"ipa\", \"eng\"]\n",
    "\n",
    "# allosaurus_models = [\"eng2102\"]\n",
    "# phone_inventory = [\"eng\"]\n",
    "\n",
    "# Download models\n",
    "for m in allosaurus_models:\n",
    "    allosaurus.bin.download_model.download_model(m)\n",
    "\n",
    "# Predict and check against gold standard\n",
    "for model, pi in itertools.product(allosaurus_models, phone_inventory):\n",
    "    model_predictions = allosaurus_predict(non_empty_test_data, model, pi)\n",
    "    model_name = f\"allosaurus_{model}_{pi}\"\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(model_name, model_predictions[PREDICTION_KEY], non_empty_test_data[\"ipa\"])\n",
    "    write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, model_name, non_empty_test_data, model_predictions, metrics\n",
    "    )\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
