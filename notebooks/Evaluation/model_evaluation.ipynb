{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "## Notebook to evaluate different models:\n",
    "- Ours Multipa\n",
    "- Ctaguchi Model\n",
    "- Allosaraus Model\n",
    "- ZIPA Model (Pending Env Issues and IceFall, K2 not on mac?)\n",
    "\n",
    "### Pending: Zipa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93c94",
   "metadata": {},
   "source": [
    "### Additional installation step for Epitran\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/panphon/featuretable.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "import datasets\n",
    "import epitran\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multipa.data_utils\n",
    "import multipa.evaluation\n",
    "\n",
    "\n",
    "DEVICE = -1  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "\n",
    "# Set up results directories\n",
    "VERBOSE_RESULTS_DIR = Path(\"../../data/timit_results/detailed_predictions\")\n",
    "AGGREGATE_METRICS_CSV = Path(\"../../data/timit_results/aggregate_metrics/all_models_eval.csv\")\n",
    "EDIT_DIST_DIR = Path(\"../../data/timit_results/edit_distances/\")\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing options\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "evaluated_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR1/FAKS0/SA1.WAV'}, 'filename': '/complete/dr1/faks0/sa1.wav', 'ipa': ' ʃ i ɦ æ d j ɝ d ɑ ɹ k s u ɾ ɪ ŋ g ɹ i s i w ɑ ʃ  w ɑ ɾ ɝ ʔ ɔ l j i ɚ '}\n"
     ]
    }
   ],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)}, \n",
    "        \"filename\": clean_filename, \n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# TODO: Evaluate on the whole dataset\n",
    "# Test with a small subset if wanted\n",
    "audio_subset = audio_dataset.select([i for i in list(range(10))])\n",
    "# audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 33.55 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:01<00:00,  6.59 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:01<00:00,  7.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 10\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR1/FAKS0/SA1.WAV', 'array': array([9.15527344e-05, 1.52587891e-04, 6.10351562e-05, ...,\n",
      "       2.44140625e-04, 3.05175781e-04, 2.13623047e-04], shape=(63488,)), 'sampling_rate': 16000}, 'filename': '/complete/dr1/faks0/sa1.wav', 'ipa': 'ʃiɦædjɝdɑɹksuɾɪŋgɹisiwɑʃwɑɾɝʔɔljiɚ'}\n",
      "Audio without speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset, \n",
    "    is_remove_space=IS_REMOVE_SPACES, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcirptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82806879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    recog = allosaurus.app.read_recognizer(model)\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        wav_path = audio[\"path\"]\n",
    "        data, sr = sf.read(wav_path)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
    "            sf.write(tmp.name, data, sr, format=\"WAV\", subtype=\"PCM_16\")\n",
    "            prediction = recog.recognize(tmp.name, phone_inventory)\n",
    "        #prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "            model_predictions.append({multipa.evaluation.PREDICTION_KEY: prediction})\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, generate_kwargs={\"language\": \"english\"}, device=DEVICE\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        print(\"preditction:\", pred)\n",
    "        result = epi.transliterate(pred)\n",
    "        ipa_predictions.append({multipa.evaluation.PREDICTION_KEY: result})\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276afb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating allosaurus. Model: eng2102 Phone inventory: eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/allosaurus/am/utils.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(str(path), map_location=torch.device('cpu'))\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 64.05 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2721.45 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 156.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating Allosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2793.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allosaurus inference and metrics compute\n",
    "allosaurus_model = \"eng2102\"\n",
    "phone_inventory = \"eng\"\n",
    "allosaurus_model_name = f\"allosaurus_{allosaurus_model}_{phone_inventory}\"\n",
    "allosaurus.bin.download_model.download_model(allosaurus_model)\n",
    "allosaurus_predictions = allosaurus_predict(audio_subset, model=allosaurus_model, phone_inventory=phone_inventory)\n",
    "allosaurus_metrics = model_evaluator.eval_non_empty_transcriptions(allosaurus_model_name, \n",
    "    allosaurus_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "# Write prediction details and edit distances\n",
    "model_evaluator.write_edit_distance_results(allosaurus_model_name, EDIT_DIST_DIR)\n",
    "multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, allosaurus_model_name, audio_subset, allosaurus_predictions, allosaurus_metrics)\n",
    "\n",
    "# Save model results for later\n",
    "print(\"Done evaluating Allosaurus\")\n",
    "evaluated_models.append(allosaurus_model_name)\n",
    "full_analysis_dataset = audio_subset.add_column(allosaurus_model_name, allosaurus_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR for model: ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 58.23 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2732.27 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 431.78ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Running ASR for model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 58.40 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 3023.14 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 468.43ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "models = [our_model, taguchi_1k]\n",
    "for model_name in models:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(audio_subset, asr_pipe, \n",
    "        num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES)\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(model_name, \n",
    "        predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics)\n",
    "    \n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2569b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaulating openai_whisper-large-v3-turbo_to_epitran\n",
      "Building pipeline and downloading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with openai/whisper-large-v3-turbo\n",
      "Transliterating with Epitran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]WARNING:epitran:lex_lookup (from flite) is not installed.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preditction:  she had your dark suit in greasy wash water all year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m model_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_to_epitran\u001b[39m\u001b[33m\"\u001b[39m.replace(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaulating\u001b[39m\u001b[33m\"\u001b[39m, model_name)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m epitran_predictions = \u001b[43mhf_model_to_epitran_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_subset\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m     11\u001b[39m metrics = model_evaluator.eval_non_empty_transcriptions(\n\u001b[32m     12\u001b[39m     model_name, epitran_predictions[PREDICTION_KEY], audio_subset[\u001b[33m\"\u001b[39m\u001b[33mipa\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, audio_subset, epitran_predictions, metrics)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mhf_model_to_epitran_predict\u001b[39m\u001b[34m(model_name, test_dataset)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m tqdm(orthography_predictions):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpreditction:\u001b[39m\u001b[33m\"\u001b[39m, pred)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     result = \u001b[43mepi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransliterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     ipa_predictions.append({multipa.evaluation.PREDICTION_KEY: result})\n\u001b[32m     36\u001b[39m predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/epitran/_epitran.py:52\u001b[39m, in \u001b[36mEpitran.transliterate\u001b[39m\u001b[34m(self, word, normpunc, ligatures)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransliterate\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, normpunc: \u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m, ligatures: \u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     44\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transliterates/transcribes a word into IPA\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m \u001b[33;03m    :param word str: word to transcribe\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    :rtype: str\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransliterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormpunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mligatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/epitran/flite.py:96\u001b[39m, in \u001b[36mFlite.transliterate\u001b[39m\u001b[34m(self, text, normpunc, ligatures)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_re.findall(text):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.letter_re.match(chunk):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         acc.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menglish_g2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m         acc.append(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/epitran/flite.py:214\u001b[39m, in \u001b[36mFliteLexLookup.english_g2p\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    211\u001b[39m     arpa_text = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Split on newlines and take the first element (in case lex_lookup\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# returns multiple lines).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m arpa_text = \u001b[43marpa_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arpa_to_ipa(arpa_text)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Orthographic to epitran models\n",
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    print(\"Evaulating\", model_name)\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, audio_subset)    \n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, audio_subset, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = audio_subset.add_column(name=model_name, column=epitran_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These models were evaluated:\", evaluated_models)\n",
    "print(\"Dataset snippet for full anslysis:\")\n",
    "print(full_analysis_dataset)\n",
    "print(full_analysis_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "\n",
    "gold_standard_df[\"clean_ipa\"] = gold_standard_df[\"ipa_transcription\"].apply(\n",
    "    lambda x: \"\".join(str(x).split())\n",
    ")\n",
    "\n",
    "gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "\n",
    "audio_subset = audio_subset.map(lambda x: {**x, \"filename\": x[\"filename\"].lower()})\n",
    "\n",
    "predictions_df = full_analysis_dataset.to_pandas()\n",
    "\n",
    "full_comparison_df = pd.merge(\n",
    "    gold_standard_df,\n",
    "    predictions_df,\n",
    "    on=\"filename\"\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df.drop(\n",
    "    columns=[\"audio_filename\", \"ipa_transcription\", \"audio\"]\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df[[\n",
    "    \"filename\", \n",
    "    \"ipa\", \n",
    "    \"ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\", \n",
    "    \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\",\n",
    "    \"allosaurus_eng2102_ipa\"\n",
    "]]\n",
    "\n",
    "print(full_comparison_df.head())\n",
    "\n",
    "output_path = Path(\"timit_subset_with_actual_and_predictions.csv\")\n",
    "full_comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged dataset saved to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = multipa.evaluate.ModelEvaluator()\n",
    "gold_col = \"ipa\"\n",
    "model_names = [our_model, taguchi_1k, \"allosaurus_eng2102_ipa\"]\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "    \n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
