{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "## Notebook to evaluate different models:\n",
    "- Ours Multipa\n",
    "- Ctaguchi Model\n",
    "- Allosaraus Model\n",
    "- ZIPA Model (Pending Env Issues and IceFall, K2 not on mac?)\n",
    "\n",
    "### Pending: Zipa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93c94",
   "metadata": {},
   "source": [
    "### Additional installation step for Epitran\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "import datasets\n",
    "import epitran\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multipa.data_utils\n",
    "import multipa.evaluation\n",
    "\n",
    "\n",
    "DEVICE = -1  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "\n",
    "# Set up results directories\n",
    "RESULTS_DIR =Path(\"../../data/timit_results\")\n",
    "VERBOSE_RESULTS_DIR = RESULTS_DIR / \"detailed_predictions\"\n",
    "AGGREGATE_METRICS_CSV = RESULTS_DIR / \"aggregate_metrics/all_models_eval.csv\"\n",
    "EDIT_DIST_DIR = RESULTS_DIR / \"edit_distances\"\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing options\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "evaluated_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV'}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': ' w ɨ d s ʌ tʃ ɨ n æ k t ɨ v ɹ ɨ f j ʉ ʒ l̩  b i j ʉ s f l̩  '}\n"
     ]
    }
   ],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)},\n",
    "        \"filename\": clean_filename,\n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# TODO: Evaluate on the whole dataset\n",
    "# Test with a small subset if wanted\n",
    "audio_subset = audio_dataset.select([i for i in list(range(10))])\n",
    "# audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 61.81 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 23.36 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 30.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 10\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV', 'array': array([-2.13623047e-04,  6.10351562e-05,  3.05175781e-05, ...,\n",
      "       -3.05175781e-05, -9.15527344e-05, -6.10351562e-05]), 'sampling_rate': 16000}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': 'wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩'}\n",
      "Audio without speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset,\n",
    "    is_remove_space=IS_REMOVE_SPACES, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcirptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82806879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    recog = allosaurus.app.read_recognizer(model)\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        wav_path = audio[\"path\"]\n",
    "        data, sr = sf.read(wav_path)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
    "            sf.write(tmp.name, data, sr, format=\"WAV\", subtype=\"PCM_16\")\n",
    "            prediction = recog.recognize(tmp.name, phone_inventory)\n",
    "        #prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "            model_predictions.append({multipa.evaluation.PREDICTION_KEY: prediction})\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, generate_kwargs={\"language\": \"english\"}, device=DEVICE\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        result = epi.transliterate(pred)\n",
    "        ipa_predictions.append({multipa.evaluation.PREDICTION_KEY: result})\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276afb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating allosaurus. Model: eng2102 Phone inventory: eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/allosaurus/am/utils.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(str(path), map_location=torch.device('cpu'))\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.72it/s]\n",
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 82.00 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2827.11 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 336.68ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating Allosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 3357.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allosaurus inference and metrics compute\n",
    "allosaurus_model = \"eng2102\"\n",
    "phone_inventory = \"eng\"\n",
    "allosaurus_model_name = f\"allosaurus_{allosaurus_model}_{phone_inventory}\"\n",
    "allosaurus.bin.download_model.download_model(allosaurus_model)\n",
    "allosaurus_predictions = allosaurus_predict(audio_subset, model=allosaurus_model, phone_inventory=phone_inventory)\n",
    "allosaurus_metrics = model_evaluator.eval_non_empty_transcriptions(allosaurus_model_name,\n",
    "    allosaurus_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "# Write prediction details and edit distances\n",
    "model_evaluator.write_edit_distance_results(allosaurus_model_name, EDIT_DIST_DIR)\n",
    "multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, allosaurus_model_name, audio_subset, allosaurus_predictions, allosaurus_metrics)\n",
    "\n",
    "# Save model results for later\n",
    "print(\"Done evaluating Allosaurus\")\n",
    "evaluated_models.append(allosaurus_model_name)\n",
    "full_analysis_dataset = audio_subset.add_column(allosaurus_model_name, allosaurus_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR for model: ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 62.01 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 5794.04 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 625.74ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Running ASR for model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 66.82 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 3377.33 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 641.43ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "models = [our_model, taguchi_1k]\n",
    "for model_name in models:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(audio_subset, asr_pipe,\n",
    "        num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES)\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(model_name,\n",
    "        predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics)\n",
    "\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaulating openai_whisper-large-v3-turbo_to_epitran\n",
      "Building pipeline and downloading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed language=english, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=english.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with openai/whisper-large-v3-turbo\n",
      "Transliterating with Epitran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\n",
      "Parameter 'function'=<function hf_model_to_epitran_predict.<locals>.<lambda> at 0x34a07e160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function hf_model_to_epitran_predict.<locals>.<lambda> at 0x34a07e160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 55.80 examples/s]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2798.81 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 146.82ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating openai_whisper-large-v3-turbo_to_epitran\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns', 'openai_whisper-large-v3-turbo_to_epitran'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Evaulating openai_whisper-large-v3_to_epitran\n",
      "Building pipeline and downloading model\n",
      "Predicting with openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Orthographic to epitran models\n",
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    print(\"Evaulating\", model_name)\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, audio_subset)\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, audio_subset, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=epitran_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These models were evaluated:\", evaluated_models)\n",
    "print(\"Dataset snippet for full anslysis:\")\n",
    "print(full_analysis_dataset)\n",
    "print(full_analysis_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = full_analysis_dataset.to_pandas()\n",
    "print(predictions_df.head())\n",
    "\n",
    "# full_comparison_df = full_comparison_df.drop(\n",
    "#     columns=[\"audio_filename\", \"ipa_transcription\", \"audio\"]\n",
    "# )\n",
    "\n",
    "# full_comparison_df = full_comparison_df[[\n",
    "#     \"filename\",\n",
    "#     \"ipa\",\n",
    "#     \"ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\",\n",
    "#     \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\",\n",
    "#     \"allosaurus_eng2102_ipa\"\n",
    "# ]]\n",
    "\n",
    "# print(full_comparison_df.head())\n",
    "\n",
    "# output_path = Path(\"timit_subset_with_actual_and_predictions.csv\")\n",
    "# full_comparison_df.to_csv(output_path, index=False)\n",
    "# print(f\"Merged dataset saved to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_col = \"ipa\"\n",
    "model_names = [our_model, taguchi_1k, \"allosaurus_eng2102_ipa\"]\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "\n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
