{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "## Notebook to evaluate different models:\n",
    "- Ours Multipa\n",
    "- Ctaguchi Model\n",
    "- Allosaraus Model\n",
    "- ZIPA Model (Pending Env Issues and IceFall, K2 not on mac?)\n",
    "\n",
    "### Pending: Zipa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93c94",
   "metadata": {},
   "source": [
    "### Additional installation step for Epitran\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "\n",
    "import multipa.data_utils\n",
    "import multipa.evaluation\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = -1  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "\n",
    "# Set up results directories\n",
    "VERBOSE_RESULTS_DIR = Path(\"../../data/timit_results/detailed_predictions\")\n",
    "AGGREGATE_METRICS_CSV = Path(\"../../data/timit_results/aggregate_metrics/all_models_eval.csv\")\n",
    "EDIT_DIST_DIR = Path(\"../../data/timit_results/edit_distances/\")\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing options\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)}, \n",
    "        \"filename\": clean_filename, \n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# Test with a small subset if wanted\n",
    "audio_subset = audio_dataset.select([i for i in list(range(10))])\n",
    "# audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset, \n",
    "    is_remove_space=IS_REMOVE_SPACES, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcirptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82806879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    recog = allosaurus.app.read_recognizer(model)\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        wav_path = audio[\"path\"]\n",
    "        data, sr = sf.read(wav_path)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
    "            sf.write(tmp.name, data, sr, format=\"WAV\", subtype=\"PCM_16\")\n",
    "            prediction = recog.recognize(tmp.name, phone_inventory)\n",
    "        #prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "            model_predictions.append({multipa.evaluation.PREDICTION_KEY: prediction})\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, generate_kwargs={\"language\": \"english\"}\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        ipa_predictions.append({multipa.evaluation.PREDICTION_KEY: result})\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276afb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:03<00:02,  1.69it/s]"
     ]
    }
   ],
   "source": [
    "# Allosaurus inference and metrics compute\n",
    "allosaurus_model = \"eng2102\"\n",
    "phone_inventory = \"eng\"\n",
    "allosaurus_model_name = f\"allosaurus_{allosaurus_model}_{phone_inventory}\"\n",
    "allosaurus.bin.download_model.download_model(allosaurus_model)\n",
    "allosaurus_predictions = allosaurus_predict(audio_subset, model=allosaurus_model, phone_inventory=phone_inventory)\n",
    "allosaurus_metrics = model_evaluator.eval_non_empty_transcriptions(allosaurus_model_name, \n",
    "    allosaurus_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "# Write prediction details and edit distances\n",
    "model_evaluator.write_edit_distance_results(allosaurus_model_name, EDIT_DIST_DIR)\n",
    "multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, allosaurus_model_name, audio_subset, allosaurus_predictions, allosaurus_metrics)\n",
    "\n",
    "# Save model results for later\n",
    "print(\"Allosaurus predictions complete\")\n",
    "audio_subset = audio_subset.add_column(allosaurus_model_name, allosaurus_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "print(audio_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "models = [our_model, taguchi_1k]\n",
    "for model_name in models:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(audio_subset, asr_pipe, \n",
    "        num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES)\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(model_name, \n",
    "        predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics)\n",
    "    \n",
    "    audio_subset = audio_subset.add_column(name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(\"Predictions for\", model_name, \"complete\")\n",
    "    print(audio_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    # Epitran\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, audio_subset)\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, non_empty_test_data, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    audio_subset = audio_subset.add_column(name=model_name, column=epitran_predictions[multipa.evaluation.PREDICTION_KEY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "\n",
    "gold_standard_df[\"clean_ipa\"] = gold_standard_df[\"ipa_transcription\"].apply(\n",
    "    lambda x: \"\".join(str(x).split())\n",
    ")\n",
    "\n",
    "gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "\n",
    "audio_subset = audio_subset.map(lambda x: {**x, \"filename\": x[\"filename\"].lower()})\n",
    "\n",
    "predictions_df = audio_subset.to_pandas()\n",
    "\n",
    "full_comparison_df = pd.merge(\n",
    "    gold_standard_df,\n",
    "    predictions_df,\n",
    "    on=\"filename\"\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df.drop(\n",
    "    columns=[\"audio_filename\", \"ipa_transcription\", \"audio\"]\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df[[\n",
    "    \"filename\", \n",
    "    \"clean_ipa\", \n",
    "    \"ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\", \n",
    "    \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\",\n",
    "    \"allosaurus_eng2102_ipa\"\n",
    "]]\n",
    "\n",
    "print(full_comparison_df.head())\n",
    "\n",
    "output_path = Path(\"timit_subset_with_actual_and_predictions.csv\")\n",
    "full_comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged dataset saved to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = multipa.evaluate.ModelEvaluator()\n",
    "gold_col = \"clean_ipa\"\n",
    "model_names = [our_model, taguchi_1k, \"allosaurus_eng2102_ipa\"]\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "    \n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
