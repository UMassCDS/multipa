{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "# TIMIT Evaluation \n",
    "\n",
    "Runs evaluation scripts on the TIMIT corpus to get phone error rates and edit distances for TIMIT (unseen data) for the following models:\n",
    "- Facebook's facebook/wav2vec2-lv-60-espeak-cv-ft \n",
    "\n",
    "For convenience, this model has been separated from the others, because it requires https://github.com/espeak-ng/espeak-ng and needs special modules to run on the Unity cluster. \n",
    "\n",
    "On unity, you need to use `--constraint=avx512` and in addition to standard python modules, make sure you load the following: \n",
    "```bash\n",
    "module load conda/latest load uri/main all/eSpeak-NG/1.50-gompi-2020a\n",
    "```\n",
    "\n",
    "Alternatively, you can install espeak locally and run this notebook on your machine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multipa.data_utils\n",
    "import multipa.evaluation\n",
    "\n",
    "\n",
    "DEVICE = -1  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "facebook = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "HF_MODEL_LIST = [facebook]\n",
    "\n",
    "# Set up results directories\n",
    "RESULTS_DIR =Path(\"../../data/timit_results\")\n",
    "VERBOSE_RESULTS_DIR = RESULTS_DIR / \"detailed_predictions\"\n",
    "AGGREGATE_METRICS_CSV = RESULTS_DIR / \"aggregate_metrics/facebook_wav2vec2-lv-60-espeak-cv-ft.csv\"\n",
    "EDIT_DIST_DIR = RESULTS_DIR / \"edit_distances\"\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing options\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "evaluated_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df[\"ipa_transcription\"] = gold_standard_df[\"ipa_transcription\"].str.replace(\"ɝ\", \"ɹ̩\")\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV'}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': ' w ɨ d s ʌ tʃ ɨ n æ k t ɨ v ɹ ɨ f j ʉ ʒ l̩  b i j ʉ s f l̩  '}\n"
     ]
    }
   ],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)},\n",
    "        \"filename\": clean_filename,\n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# TODO: Evaluate on the whole dataset\n",
    "# Test with a small subset if wanted\n",
    "audio_subset = audio_dataset.select([i for i in list(range(10))])\n",
    "# audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 47.20 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 17.71 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 10/10 [00:00<00:00, 30.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 10\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV', 'array': array([-2.13623047e-04,  6.10351562e-05,  3.05175781e-05, ...,\n",
      "       -3.05175781e-05, -9.15527344e-05, -6.10351562e-05]), 'sampling_rate': 16000}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': 'wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩'}\n",
      "Audio without speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset,\n",
    "    is_remove_space=IS_REMOVE_SPACES, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcirptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef262f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR for model: facebook/wav2vec2-lv-60-espeak-cv-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "espeak not installed on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m clean_model_name \u001b[38;5;241m=\u001b[39m multipa\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mclean_model_name(model_name)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning ASR for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m asr_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mautomatic-speech-recognition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m predictions_dataset \u001b[38;5;241m=\u001b[39m multipa\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mget_clean_predictions(\n\u001b[1;32m      7\u001b[0m     audio_subset, asr_pipe, num_proc\u001b[38;5;241m=\u001b[39mNUM_PROC, is_remove_space\u001b[38;5;241m=\u001b[39mIS_REMOVE_SPACES\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute all metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/pipelines/__init__.py:994\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    992\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 994\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:896\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m         )\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2291\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2289\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2525\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2525\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2530\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py:136\u001b[0m, in \u001b[0;36mWav2Vec2PhonemeCTCTokenizer.__init__\u001b[0;34m(self, vocab_file, bos_token, eos_token, unk_token, pad_token, phone_delimiter_token, word_delimiter_token, do_phonemize, phonemizer_lang, phonemizer_backend, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphonemizer_backend \u001b[38;5;241m=\u001b[39m phonemizer_backend\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_phonemize:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphonemizer_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py:185\u001b[0m, in \u001b[0;36mWav2Vec2PhonemeCTCTokenizer.init_backend\u001b[0;34m(self, phonemizer_lang)\u001b[0m\n\u001b[1;32m    182\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphonemizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BACKENDS\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m \u001b[43mBACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphonemizer_backend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphonemizer_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_switch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremove-flags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/phonemizer/backend/espeak/espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mset_voice(language)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_stress \u001b[38;5;241m=\u001b[39m with_stress\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/phonemizer/backend/espeak/base.py:39\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak \u001b[38;5;241m=\u001b[39m EspeakWrapper()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mlibrary_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/multipa/lib/python3.11/site-packages/phonemizer/backend/base.py:77\u001b[0m, in \u001b[0;36mBaseBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# ensure the backend is installed on the system\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not installed on your system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname()))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logger\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializing backend \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion()))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: espeak not installed on your system"
     ]
    }
   ],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "for model_name in HF_MODEL_LIST:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(\n",
    "        audio_subset, asr_pipe, num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES\n",
    "    )\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(\n",
    "        name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY]\n",
    "    )\n",
    "    print(full_analysis_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These models were evaluated:\", evaluated_models)\n",
    "print(\"Dataset snippet for full anslysis:\")\n",
    "print(full_analysis_dataset)\n",
    "print(full_analysis_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = full_analysis_dataset.to_pandas()\n",
    "print(\"predictions_df snippet\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "full_comparison_df = predictions_df.drop(\n",
    "    columns=[\"audio\"]\n",
    "    )\n",
    "\n",
    "print(\"full_comparison_df snippet\")\n",
    "print(full_comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_col = \"ipa\"\n",
    "model_names = evaluated_models\n",
    "model_eval = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "\n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_facebook_wav2vec2-lv-60-espeak-cv-ft_evaluation_summary.csv\", index=False)\n",
    "print(\n",
    "    \"Average evaluation metrics per model saved to timit_facebook_wav2vec2-lv-60-espeak-cv-ft_evaluation_summary.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_facebook_wav2vec2-lv-60-espeak-cv-ft_comparison.csv\", index=False)\n",
    "print(\n",
    "    \"Dialect evaluation complete. Results saved to timit_dialect_facebook_wav2vec2-lv-60-espeak-cv-ft_comparison.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
