{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "## Notebook to evaluate different models:\n",
    "- Ours Multipa\n",
    "- Ctaguchi Model\n",
    "- Allosaraus Model\n",
    "- ZIPA Model\n",
    "\n",
    "### Pending: Allosaraus and ZIPA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import Audio, Dataset\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "import multipa\n",
    "import multipa.data_utils\n",
    "import multipa.evaluate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = -1  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "\n",
    "# Models Evaluating\n",
    "our_model = \"ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n"
     ]
    }
   ],
   "source": [
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "\n",
    "data = [\n",
    "    {\"audio\": {\"path\": str(p)}, \"filename\": \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()}\n",
    "    for p in timit_wavs\n",
    "]\n",
    "\n",
    "audio_dataset = Dataset.from_list(data)\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Test with a small subset if wanted\n",
    "audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR for model: ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "Running ASR for model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "models = [our_model, taguchi_1k]\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    \n",
    "    asr_pipe = pipeline(\"automatic-speech-recognition\", model=model_name, device=device)\n",
    "    \n",
    "    predictions = asr_pipe(audio_subset[\"audio\"])\n",
    "    \n",
    "    cleaned_predictions = [\n",
    "        multipa.data_utils.clean_text(x, is_remove_space=True, text_key=\"text\")[\"text\"]\n",
    "        for x in predictions\n",
    "    ]\n",
    "\n",
    "    audio_subset = audio_subset.add_column(name=model_name, column=cleaned_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 58.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filename                              clean_ipa  \\\n",
      "0    /complete/dr4/mmdm0/sa1.wav       ʃiædjɚdɑɹksʉɾɨngɹiziwɔʃwɑɾɚɔljɪɹ   \n",
      "1    /complete/dr4/mmdm0/sa2.wav         doʊɾ̃æsmiɾɨkɪɹiɛɾ̃ɔliɹæglʌkðæt   \n",
      "2  /complete/dr4/mmdm0/si681.wav         wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩   \n",
      "3  /complete/dr4/mmdm0/sx231.wav                           ʔɑʔɑɾ̃ɚmɑmɑm   \n",
      "4  /complete/dr4/mmdm0/sx411.wav  bʌɾɚskɑtʃfʌdʒɡoʊzwɛlwəðvəɾ̃ɪləaɪskɹim   \n",
      "\n",
      "  ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa  \\\n",
      "0                ʃiædjɹ̩dɑɹksuɾɪnɡɹiziwʌʃwɑɾɹ̩aʊljiɹ          \n",
      "1                   doʊɾ̃æskmiɾɪkɛɹiɛnoʊliɹæɡlaɪkðæʔ          \n",
      "2                     wɪθsʌtʃɪnæktʌvɹɪfjuzl̩bijusfl̩          \n",
      "3                                        ʌɑɾɹ̩mɑmmɑm          \n",
      "4               bɛɾɹ̩skʌtʃfʌdʒɡoʊzwɛlwʌðvʌnɛlæskɹiɪŋ          \n",
      "\n",
      "  ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns  \n",
      "0                  ɕiærjydɑːkjsɯyringɻiziwɑʃwɑrɑojiː      \n",
      "1                     nooneːsmirukeɻienɒlujɻæːɡlɛkte      \n",
      "2                       wɨtsat͡ɕinaktɨrɨfizo̞bɨjusfo      \n",
      "3                                          ɛanmamaːm      \n",
      "4           bɛrørskaːt͡ʃfɒjd͡ʑkɛuzvɛotvneːlaaːskrejɲ      \n",
      "Merged dataset saved to /Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/Evaluation/Evaluation/timit_subset_with_actual_and_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "\n",
    "gold_standard_df[\"clean_ipa\"] = gold_standard_df[\"ipa_transcription\"].apply(\n",
    "    lambda x: \"\".join(str(x).split())\n",
    ")\n",
    "\n",
    "gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "\n",
    "audio_subset = audio_subset.map(lambda x: {**x, \"filename\": x[\"filename\"].lower()})\n",
    "\n",
    "predictions_df = audio_subset.to_pandas()\n",
    "\n",
    "full_comparison_df = pd.merge(\n",
    "    gold_standard_df,\n",
    "    predictions_df,\n",
    "    on=\"filename\"\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df.drop(\n",
    "    columns=[\"audio_filename\", \"ipa_transcription\", \"audio\"]\n",
    ")\n",
    "\n",
    "full_comparison_df = full_comparison_df[[\n",
    "    \"filename\", \n",
    "    \"clean_ipa\", \n",
    "    \"ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\", \n",
    "    \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "]]\n",
    "\n",
    "print(full_comparison_df.head())\n",
    "\n",
    "output_path = Path(\"timit_subset_with_actual_and_predictions.csv\")\n",
    "full_comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged dataset saved to {output_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialect groups found: ['DR4']\n",
      "\n",
      "Evaluating model: ginic/data_seed_bs64_4_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "\n",
      "Evaluating model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n",
      "Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\n",
      "\n",
      "Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "model_eval = multipa.evaluate.ModelEvaluator()\n",
    "gold_col = \"clean_ipa\"\n",
    "model_names = [our_model, taguchi_1k]\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    \n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "    \n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"\\nDialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
